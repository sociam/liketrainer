% This template has been tested with LLNCS DOCUMENT CLASS -- version 2.20 (24-JUN-2015)

%"runningheads" enables:
%  - page number on page 2 onwards
%  - title/authors on even/odd pages
%This is good for other readers to enable proper archiving among other papers and pointing to content.
%Even if the title page states the title, when printed and stored in a folder, when blindly opening the folder, one could hit not the title page, but an arbitrary page. Therefore, it is good to have title printed on the pages, too.
\documentclass[runningheads,a4paper]{llncs}

%cmap has to be loaded before any font package (such as cfr-lm)
\usepackage{cmap}
\usepackage[T1]{fontenc}

\usepackage{graphicx}

%Even though `american`, `english` and `USenglish` are synonyms for babel package (according to https://tex.stackexchange.com/questions/12775/babel-english-american-usenglish), the llncs document class is prepared to avoid the overriding of certain names (such as "Abstract." -> "Abstract" or "Fig." -> "Figure") when using `english`, but not when using the other 2.
%english has to go last to set it as default language
\usepackage[ngerman,english]{babel}
%Hint by http://tex.stackexchange.com/a/321066/9075 -> enable "= as dashes
\addto\extrasenglish{\languageshorthands{ngerman}\useshorthands{"}}

%better font, similar to the default springer font
%cfr-lm is preferred over lmodern. Reasoning at http://tex.stackexchange.com/a/247543/9075
\usepackage[%
rm={oldstyle=false,proportional=true},%
sf={oldstyle=false,proportional=true},%
tt={oldstyle=false,proportional=true,variable=true},%
qt=false%
]{cfr-lm}
%
%if more space is needed, exchange cfr-lm by mathptmx
%\usepackage{mathptmx}

%for demonstration purposes only
\usepackage[math]{blindtext}

%Sorts the citations in the brackets
%It also allows \cite{refa, refb}. Otherwise, the document does not compile.
%  Error message: "White space in argument"
\usepackage{cite}


%% If you need packages for other papers,
%% START COPYING HERE
%% COPY ALSO cmap and fontenc from lines 10 to 12

%extended enumerate, such as \begin{compactenum}
\usepackage{paralist}

%put figures inside a text
%\usepackage{picins}
%use
%\piccaptioninside
%\piccaption{...}
%\parpic[r]{\includegraphics ...}
%Text...

%for easy quotations: \enquote{text}
\usepackage{csquotes}

%enable margin kerning
\usepackage{microtype}

%tweak \url{...}
\usepackage{url}
%\urlstyle{same}
%improve wrapping of URLs - hint by http://tex.stackexchange.com/a/10419/9075
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
%nicer // - solution by http://tex.stackexchange.com/a/98470/9075
%DO NOT ACTIVATE -> prevents line breaks
%\makeatletter
%\def\Url@twoslashes{\mathchar`\/\@ifnextchar/{\kern-.2em}{}}
%\g@addto@macro\UrlSpecials{\do\/{\Url@twoslashes}}
%\makeatother

%diagonal lines in a table - http://tex.stackexchange.com/questions/17745/diagonal-lines-in-table-cell
%slashbox is not available in texlive (due to licensing) and also gives bad results. This, we use diagbox
%\usepackage{diagbox}

%required for pdfcomment later
\usepackage{xcolor}


%enable nice comments
%this also loads hyperref
\usepackage{pdfcomment}
%enable hyperref without colors and without bookmarks
\hypersetup{hidelinks,
   colorlinks=true,
   allcolors=black,
   pdfstartview=Fit,
   breaklinks=true}
%enables correct jumping to figures when referencing
\usepackage[all]{hypcap}

\newcommand{\commentontext}[2]{\colorbox{yellow!60}{#1}\pdfcomment[color={0.234 0.867 0.211},hoffset=-6pt,voffset=10pt,opacity=0.5]{#2}}
\newcommand{\commentatside}[1]{\pdfcomment[color={0.045 0.278 0.643},icon=Note]{#1}}

%compatibality with packages todo, easy-todo, todonotes
\newcommand{\todo}[1]{\commentatside{#1}}
%compatiblity with package fixmetodonotes
\newcommand{\TODO}[1]{\commentatside{#1}}

%enable \cref{...} and \Cref{...} instead of \ref: Type of reference included in the link
\usepackage[capitalise,nameinlink]{cleveref}
%Nice formats for \cref
\crefname{section}{Sect.}{Sect.}
\Crefname{section}{Section}{Sections}

\usepackage{xspace}
%\newcommand{\eg}{e.\,g.\xspace}
%\newcommand{\ie}{i.\,e.\xspace}
\newcommand{\eg}{e.\,g.,\ }
\newcommand{\ie}{i.\,e.,\ }

%introduce \powerset - hint by http://matheplanet.com/matheplanet/nuke/html/viewtopic.php?topic=136492&post_id=997377
\DeclareFontFamily{U}{MnSymbolC}{}
\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
\DeclareFontShape{U}{MnSymbolC}{m}{n}{
    <-6>  MnSymbolC5
   <6-7>  MnSymbolC6
   <7-8>  MnSymbolC7
   <8-9>  MnSymbolC8
   <9-10> MnSymbolC9
  <10-12> MnSymbolC10
  <12->   MnSymbolC12%
}{}
\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%% END COPYING HERE


\begin{document}

\title{Like trainer, like bot: inheritance of bias in algorithmic content moderation}
%If Title is too long, use \titlerunning
%\titlerunning{Short Title}

% %Single insitute
% \author{Firstname Lastname \and Firstname Lastname}
% %If there are too many authors, use \authorrunning
% %\authorrunning{First Author et al.}
% \institute{Institute}

% %Multiple insitutes
% %Currently disabled
% %
% \iffalse
%Multiple institutes are typeset as follows:
\author{Reuben Binns, Max Van Kleek, Nigel Shadbolt\inst{1} \and Michael Veale\inst{2} }
%If there are too many authors, use \authorrunning
%\authorrunning{First Author et al.}

\institute{
Department of Computer Science, University of Oxford\\
\email{...}\and
StEAPP, UCL\\
\email{...}
}
\fi
			
\maketitle

\begin{abstract}
The internet has become a central medium through which 'networked publics' express their opinions and engage in debate. But offensive, aggressive comments and personal attacks can inhibit participation. Recent proposals for automated content moderation aim to overcome this with machine learning classifiers trained on large corpora of texts manually labelled by human raters as offensive, aggressive or personally attacking. While such systems could help encourage more civil debate, the normative boundaries they aim to navigage are inherently contestable, subject to the idiosyncratic norms of the human raters who provide the training data. This paper provides an initial investigation of the effects of such normative differences on algorithmic content moderation, by way of a case study using an existing dataset of comments labelled for offence. We train classifiers on comments labelled by different demographic subsets (men and women) to understand how differences in conceptions of offence between these groups might affect the performance of the resulting models on various corpora. We introduce a novel concept of 'unfairness' for automated text classification tasks where the concepts at stake are inherently normative and contestable, and use it to assess the bias of the 'gendered' classifiers. Based on this case study, we discuss some of the ethical choices facing designers of algorithmic moderation systems, and the platforms that deploy them.
\end{abstract}

\begin{keywords}
political science
algorithmic accountability
machine learning
online abuse
discussion platforms
\end{keywords}

\section{Introduction}

Online platforms are increasingly the place where we express our opinions and engage in debate. They have been called the new `curators of public discourse'~\cite{gillespie2010politics}, and the digital extension of the public sphere~\cite{dahlberg2001internet}. Social media, news websites, and question / answer forums provide significant opportunities allow their users to express views and engage in deliberation with a diverse range of peers ~\cite{halpern2013social}. However, aggressive, offensive or bullying comments can stifle debate and drive people away from participating in such discussions. Left unchecked, they may also lead to calls for intervention by regulators or law enforcement authorities. As a result, many platforms have terms of use and content policies to define the bounds of acceptable discourse, and employ active measures to enforce them ~\cite{ksiazek2015civil}.
 
Defining company policies and the standards and norms that underpin them might be within the remit of company executives, or undertaken in consultation with users and other stakeholders. Platforms with large volumes of video, like Facebook and Youtube have teams dedicated to identifying and removing offensive content. Many platforms also rely on users flagging content, both as a means of detection and as a 'rhetorical justification' for censorship ~\cite{crawford2016flag}. In some cases, moderation privileges are granted to particular volunteers, who may be self-appointed (e.g. Reddit), appointed through semi-democratic processes (e.g Wikipedia), or implicitly through reputation developed on a platform (e.g. StackExchange). Community norms defining acceptability are rarely static, consistent or uncontested. The discussion forum Reddit features many sub-fora in which different norms hold, leading to frequent arguments between users, staff and executives across sub-fora over what kinds of posts should be allowed ~\cite{centivany2016values}. What counts as acceptable is therefore always a subjective matter, particular to a platform and even to particular sub-groups within it.

Whether it is performed by employees, external agencies, users or volunteers, moderation is a primarily human endeavour. However, with the quantity of content appearing on many platforms, manually vetting each individual item can be very costly, driving interest in technical solutions which could automate harmful content detection. A common approach in the past has been to apply automatic detection of abusive terms based on manually curated blacklists of banned words, but maintaining effective lists proved difficult, particularly as language and norms change, and as users learn to game deployed systems. As a response, researchers and technology companies have proposed novel algorithmic means of moderating such content. By training machine learning algorithms on large corpora of texts manually labelled for aggression, offence or abuse, they aim to create automatic classification systems to flag harmful comments. One such initiatives is Perspective API, from Google.\footnote{See https://www.perspectiveapi.com/} According to the project description, a platform could receive a score which predicts the ‘impact a comment might have on a conversation’, which could be used ‘to give realtime feedback to commenters or help moderators do their job’. Similar efforts are being pursued by other platforms, including Twitter, a microblogging platform, and Disqus, a third party comment plugin provider.\footnote{https://blog.disqus.com/first-steps-to-curbing-toxicity, https://www.recode.net/2017/2/7/14528084/twitter-abuse-safety-features-update}.
 
While such automated content moderation might help lighten the burden on human moderators, automated content moderation with manually labelled data can only be based on the collective judgements regarding norms of offense of the people whose ratings provide training data for the classifier. Where multiple implicit or explicit communities exist — particularly where participation in labelling is not balanced — this might penalise particular types of content or communication, such as political views or vernacular. The norms of these raters risk being imposed on all users of the platform, potentially affecting the balance and diversity of participation.

This is compounded in cases where training data is de-contextualised from the domain of application. Such decontextualisation might occur as, in an effort to build more sophisticated classifiers, data from multiple platforms are combined. New companies interested in using automated content moderation may have no choice but to use models built from data collected elsewhere. Yet even if the training data is only taken from where the moderation is occurring, it might introduce historical biases or patterns incompatible with the changing nature of community norms, which would then be reproduced. Finally, in cases where the community standards are in flux, or contested between different stakeholders, the question of whether an automatic abuse classifier is ‘accurate’ is likely to impinge upon pre-existing platform conflicts.
 
This paper provides an initial investigation of the effects of potential bias in algorithmic content moderation. As an illustration of the potential risks, we experiment with a series of text classifiers using an existing dataset of 100,000 Wikipedia comments manually scored for 'toxicity', ‘aggression’ and ‘personal attacks’. In order to examine how differences between people's norms of offence might result in different classifiers, we built different classifiers from demographically distinct subsets of the population responsible for labelling the training data. Specifically, we focus on gender as a demographic variable which may be associated with differences in judgements about offence \footnote{We chose gender as a relevant demographic primarily due to the ease with which statistically balanced samples can be drawn compared to the other variables (age, education level); we do not assume or intend to establish any general conclusions about gender and offence}.
 
This case study aims to illustrate methods and metrics for exploring bias in text classification tasks where the learned concept is inherently contestable; we also use it to reflect on a range of ethical considerations that should be taken into account by designers of algorithmic moderation systems, and the platforms that deploy them. As algorithmic content moderation approaches become more pervasive, the platforms deploying them will face ethical choices with significant implications for the development of community discussions and the digital public sphere.


\section{Background and Related work}

league of legends online gaming. started on wikipedia - 63m english talk pages. wishlist.
Generic work on online discussions.

hate speech ~\cite{gagliardone2015countering}

online
harassment, and cyberbullying of kids ~\cite{schrock2011problematic,tokunaga2010following}.
 "A recent Pew Research Center study defines online
harassment to include being: called offensive names, purposefully
embarrassed, stalked, sexually harassed, physically threatened,
and harassed in a sustained manner " ~\cite{wolak2007does}.
personal attacks and abuse can suppress the free speech of the victim.

\subsection{Automated detection}

detecting hate speech ~\cite{warner2012detecting,yin2009detection,sood2012automatic}

SVM on sentiment and context

cyberbullying detection based on attack type (sexuality, race, intelligence) ~\cite{dinakar2011modeling}

character-level ngrams ~\cite{nobata2016abusive}

\subsection{Algorithmic bias}

These automated systems rely on machine learning. machine learning can be biased.
DADM and FAT-ML.
mostly concerned with fairness in terms of non-discrimination. race, gender. as yet, not applied to disparities in abuse detection.

What kinds of bias might apply in this case?

What some people think is 'abuse' is just partisan disagreement. in a study of news platform comment section moderation, Diakopoulos and Naaman found that media organisations acknowledge that their moderators may bring their own biases to the evaluation of standards~\cite{diakopoulos2011towards}. Sometimes users flag comments as abusive when other users might judge them as OK; the differences may be due to political partisan divides. As one moderator said; "what one person thinks racially prejudiced, another may think is a criticism of culture. That’s usually the toughest question".

An important difference between this case and the typical contexts that have previously been studied in DADM is that the bias is not against particular demographic groups per se (e.g. gender, race, age), but but rather against particular definitions of offense. However, it is also likely that individuals within certain demographic groups are likely to share definitions of offense, at least to some extent. For example, various studies report gender differences regarding offensive language ~\cite{johnson1985sex,sutton2001bitches,jay1992cursing}. Therefore, in talking about bias in offense classification, we are not talking about bias directly affecting individuals from a certain protected group (for instance, disportionate numbers of women being denied loans). Rather, we are talking about bias towards or against definitions of offense which are more strongly associated with one group over another. For instance, a classifier might more often classify comments as 'offensive' according to definitions of offense that are more prevalent amongst men, than alternative definitions that are more prevalent amonst women. If certain populations have particular definitions of offense, and an automated system more often classifies comments in accordance with one such definition rather than another, we might say that the system is unfair to those who favour the other definition.

Furthermore, conversational environments have norms of acceptability which do not exist in a vacuum - they are reinforced by prior norms, but also malleable. Previous research on online comments has found that by intervening in certain ways, news organizations can affect the deliberative behavior of commenters~\cite{stroud2015changing}. By altering the kinds of comments a user, they can determine what kinds of comments they post (e.g. thoughtful or thoughtless) ~\cite{sukumaran2011normative}. Finally, nasty comments can lead to negative perceptions of the content they comment on ~\cite{anderson2014nasty}.


\section{Key questions and hypotheses}

% one way to do this would be to test whether these systems have disparate effects on certain populations. do certain ethnicities, genders, ages, get censored more often than others?

% however, this is tricky. it is feasible to detect disparate impact, but this doesn't tell us whether disparities are unfair. we want to know if some such populations just are 'more offensive', or if they are in some sense disproportionately misclassified. for that we need some notion of false positive and false negative rates. we need some 'ground truth'.

Our general question is: how do latent norms and biases in training data affect the operation of offense detection systems? In order to measure this empirically, we need to define what we mean by bias and fairness. What would it mean for an offence classifier to be fair / unfair?

The usual way to evaluate a classifier is to define a loss function, which measures the extent to which its predictions are the same as the ground truth of the phenomena of interest. Normally, we are only interested in one version of the ground truth. In this case, we want to measure biases between classifiers relative to different definitions of offense; i.e. multiple 'ground truths'. To this end, we define the following terms.

For any corpus of comments C in natural language [c1, c2, ... cn], and for any definition of offence D, there is some distribution of offense labels L (0 = 'not offensive', 1 ='offensive'), over C, that are a function of D. For example, definition D might have a distribution of labels L over corpus C, D(L,C) = {L(c1) : 0, L(c2) : 1, ... L(cN) : 0}.

A classifier CL is biased​ against definition D1 and in favour of definition D2, with regards to corpus C, to the extent that CL's offense labels CL(L,C) are 'closer' to D1(L,C) than they are to D2(L,C). That is, we define compare two different loss functions, corresponding to D1 and D2, to determine if CL is biased towards D1 or D2. There are various different ways to measure loss functions, and therefore how 'close' a distribution of offence labels is to a given definition of offense. These are discussed in the results section below.


% old hypotheses:

% General classification bias: A mixed-demographic classifier will have different rates of abuse classification for male / female authors and young / old authors.
% Demographics affect classification sensitivity: classifiers trained by same-gender / similar-age annotators will have different overall rates of abuse detection compared to a classifier trained by mixed-gender / mixed-age annotators.
% Demographics affect distribution of bias: classifiers trained by annotators of different genders / different ages will exhibit significantly higher rates of disagreement over classifications.
 
\section{Datasets}
Multiple data sets were used in this study, for training and bias detection purposes.

\subsection{Wikipedia Talk Annotations}

In order to train our classifiers, we used an existing dataset from the Wikipedia Detox project. It features 100,000 annotations of Wikipedia talk page comments manually labelled by crowd-workers using the Crowdflower platform. Each comment is labelled by 10 workers on three features; ‘toxicity’, ‘personal attack’ and ‘aggression’. Each worker gives a score between -2 (very toxic / aggressive / personal attack) and 2 (benign). If the average of 10 workers’ scores for a particular comment is below 0, that comment is classified as toxic.  
Generalisation test data
We also used another dataset, in order to evaluate the generalisation of our classifiers to a different context. We used the Impermium dataset released on Kaggle, which features posts by users from a range of sources, including “news commenting sites, magazine comments, message boards, blogs, text messages”. These have been manually rated as ‘insult’ (1) or ‘neutral’ (0). kaggle.com/c/detecting-insults-in-social-commentary/data



\section{Methodology}

We began with exploratory analysis of the data.

original authors report high level of inter-annotator agreement - Krippendorf’s alpha score of 0.45"

we measued agreement within and bvetween genders.

We then trained using a dataset of manually scored comments.

we trained a classifier along the lines of the wikipedia detox project
(maybe: we also benchmarked against the jigsaw perspective API)

evaluated using the ROC / AUC.
https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve

We used a bootstrapping method to sample annotators. For each comment which had both male and female raters, we selected 10 male / female annotators at random, with replacement. We then took the average toxicity/personal attack/abuse rating for these 10 sampled raters. 

used this to generate 10 different sets of training data for each gender.

These 10 different sets of training data were used to train 10 different text classifiers to identify comments that are toxic/personal attack/abuse.

We then tested these classifiers against four different sets of test data.

- previously unseen comments withheld from the detox dataset, labelled by a mixture of males and females.

- previously unseen comments withheld from the detox dataset that had been labelled by males, randomly sampled using the same bootstrap method.

- previously unseen comments withheld from the detox dataset that had been labelled by females, randomly sampled using the same bootstrap method.

-previously unseen comments from a different dataset, of twitter posts labelled 'offensive' or 'inoffensive' by a different set of crowd workers whose demographics are unknown.
% (i haven't done mixed yet, not sure its necessary).


NB: we aren't claiming that gender is necessarily a strong determinant of norms about offense, nor are we making any claim about the origins (whether environmental, genetic or otherwise) of any putative gender differences in norms about offense, nor any normative endorsement of a particular set of norms about offense. Gender is just an easily understood and accessible demographic attribute of the labelling population, which we hypothesised would exhibit some differences between demographics. Our aim is not to essentialise gender differences. just using gender as one example of socially constructed distinction which may correlate with different norms about offence.

\section{Results}





-

structure

1. exploratory data analysis.

krippendorffs alpha - demographic subgroups differ in their judgements.
women have more diversity in what they consider offensive

2. differences in strength of coefficients between models

we compared the coefficients between the male and female classifiers. We took the features used by the classifiers, and calculated their average coefficient across the 10 classifiers created for each gender. 

3. classifier performance


-

and 'fairness'

explanation of the error rate comparison: we ask 'how well does this classifier do in a world where offense is defined by women, compared to a world where offense is defined by men?'. the comparison between error rates can be used to define a gender-specific notion of classifier 'fairness'. If a classifier performs worse according to female-defined offense, than it does compared to male-defined offense, it can be said to be 'unfair to women' in the sense that it is worse at tracking their collective judgements about offense.



so looks like both male and female-trained classifiers have a higher error rate on female-labelled test data than male-labelled test data. In other words, both are 'unfair' to women, in the sense that they are worse at replicating women's collective judgements about offense than men's. However, the male-trained classifiers have a higher disparity in type I error rates between female vs male-labelled test data than female-trained classifiers. in other words, compared to bots trained by women, bots trained by men are more likely to mis-classify comments that women think are inoffensive as offensive. Both male and female-trained bots are worse at capturing women's collective judgements about offense than men's; but for male-trained bots, the disparity is greater.

As established in the previous section, the female raters broader divergence in labelling probably explains why the classifiers are generally worse at predicting female labels.

\section{Discussion}

we then discuss the significance of our findings.

the consequences go deep into methodological questions in political philosophy.

maybe the crowd-workers’ notions of toxicity are partisanal?
but maybe lib / con are just more offensive?

first, can toxicity be separated from partisanship in a way which does not beg the question?
second, what is the right amount of diversity in a free society?
How can we get past biases in the training data?

to this end, we re-trained the model using only demographic subsets of the crowd-workers (gender, age, education). These alternative models yielded differential disparate impacts on partisan groups. (to test their significance, we also trained multiple randomly drawn subsets to obtain an expected observation for the null hypothesis).

potential partisan bias mitigation strategies (sub-class of discrimination-aware data mining).

our conclusion is not that automated systems are necessarily a bad idea. but rather that because they are never neutral, there are difficult choices to be made.


\section{Conclusion and Outlook}

\subsubsection*{Acknowledgments}
...

In the bibliography, use \texttt{\textbackslash textsuperscript} for ``st'', ``nd'', ...:
E.g., \enquote{The 2\textsuperscript{nd} conference on examples}.
When you use \href{https://www.jabref.org}{JabRef}, you can use the clean up command to achieve that.
See \url{https://help.jabref.org/en/CleanupEntries} for an overview of the cleanup functionality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs03}
\bibliography{refs}

All links were last followed on October 5, 2014.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}